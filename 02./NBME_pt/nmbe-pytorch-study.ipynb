{"cells":[{"cell_type":"markdown","metadata":{},"source":["# NBME competition notebook\n","\n","- **This version uses Internet connection for study, thus needs to be adjusted for submission.**\n","- This notebook is based on 2 notebooks:\n","    - https://www.kaggle.com/code/tanyadayanand/nbme-bert-base-uncased-using-pytorch\n","    - https://prgms.tistory.com/73\n","    \n","    Thanks to both authors!\n","- This notebook uses wandb for logging and learning rate experiment. Also, it uses pretrained model from private notebook, which again, need to be adjusted for submission.\n","- yaml file is used to manage configuration throughout the notebook.\n","- Cosine Annealing Warmup Restarts and 5 Fold Cross Validation is used to better evaluation.\n","\n","## Contents\n","- Import Lib\n","- ConfigManager\n","    - fix SEED\n","    - Load Data\n","- Dataset\n","- Model\n","    - AverageMeter\n","- Training\n","- Inference"]},{"cell_type":"markdown","metadata":{},"source":["## Import Lib"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:00:37.228402Z","iopub.status.busy":"2022-05-26T10:00:37.227917Z","iopub.status.idle":"2022-05-26T10:00:57.005782Z","shell.execute_reply":"2022-05-26T10:00:57.004723Z","shell.execute_reply.started":"2022-05-26T10:00:37.228304Z"},"trusted":true},"outputs":[],"source":["!pip install -q adamp\n","!pip install -q prettyprinter"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:00:57.008507Z","iopub.status.busy":"2022-05-26T10:00:57.008076Z","iopub.status.idle":"2022-05-26T10:00:59.618187Z","shell.execute_reply":"2022-05-26T10:00:59.617096Z","shell.execute_reply.started":"2022-05-26T10:00:57.008464Z"},"trusted":true},"outputs":[],"source":["import os\n","import math\n","import time\n","import tqdm\n","import yaml\n","import torch\n","import random\n","import warnings\n","import tokenizers\n","import numpy as np\n","import pandas as pd\n","import transformers\n","\n","from adamp import AdamP\n","from tqdm.auto import tqdm\n","from ast import literal_eval\n","from easydict import EasyDict\n","from prettyprinter import cpprint\n","from torch.optim.lr_scheduler import _LRScheduler\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","from sklearn.model_selection import KFold, train_test_split"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:00:59.627085Z","iopub.status.busy":"2022-05-26T10:00:59.624179Z","iopub.status.idle":"2022-05-26T10:00:59.634001Z","shell.execute_reply":"2022-05-26T10:00:59.633076Z","shell.execute_reply.started":"2022-05-26T10:00:59.627020Z"},"trusted":true},"outputs":[],"source":["os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:00:59.643090Z","iopub.status.busy":"2022-05-26T10:00:59.640081Z","iopub.status.idle":"2022-05-26T10:01:19.875413Z","shell.execute_reply":"2022-05-26T10:01:19.874618Z","shell.execute_reply.started":"2022-05-26T10:00:59.643016Z"},"trusted":true},"outputs":[],"source":["WANDB = True\n","\n","if WANDB:\n","    from kaggle_secrets import UserSecretsClient\n","    user_secrets = UserSecretsClient()\n","    secret_value = user_secrets.get_secret(\"wandb\")\n","    os.environ[\"WANDB_API_KEY\"] = secret_value\n","\n","    !pip -q install wandb\n","    !wandb login \n","\n","    import wandb\n","    wandb.init('nbme-study')"]},{"cell_type":"markdown","metadata":{},"source":["## ConfigManager\n","- using .yaml file\n","- test on different configs"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:19.877685Z","iopub.status.busy":"2022-05-26T10:01:19.877319Z","iopub.status.idle":"2022-05-26T10:01:19.892437Z","shell.execute_reply":"2022-05-26T10:01:19.891628Z","shell.execute_reply.started":"2022-05-26T10:01:19.877644Z"},"trusted":true},"outputs":[],"source":["class YamlConfigManager:\n","    def __init__(self, config_file_path='../input/config/config.yaml', config_name='base'):\n","        super().__init__()\n","        self.values = EasyDict()\n","        if config_file_path:\n","            self.config_file_path = config_file_path\n","            self.config_name = config_name\n","            self.reload()\n","            \n","    def reload(self):\n","        self.clear()\n","        if self.config_file_path:\n","            with open(self.config_file_path, 'r') as f:\n","                self.values.update(yaml.safe_load(f)[self.config_name])\n","                \n","    def clear(self):\n","        self.values.clear()\n","        \n","    def update(self, yaml_dict):\n","        for k1, v1 in yaml_dict.items():\n","            if isinstance(v1, dict):\n","                for k2, v2 in v1.items():\n","                    if isinstance(v2, dict):\n","                        for k3, v3 in v2.items():\n","                            self.values[k1][k2][k3] = v3\n","                    else:\n","                        self.values[k1][k2] = v2\n","            else:\n","                self.values[k1] = v1\n","                \n","    def export(self, save_file_path):\n","        if save_file_path:\n","            with open(save_file_path, 'w') as f:\n","                yaml.dump(dict(self.values), f)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:19.895028Z","iopub.status.busy":"2022-05-26T10:01:19.894298Z","iopub.status.idle":"2022-05-26T10:01:19.903849Z","shell.execute_reply":"2022-05-26T10:01:19.903108Z","shell.execute_reply.started":"2022-05-26T10:01:19.894989Z"},"trusted":true},"outputs":[],"source":["# config.yaml\n","\n","# base:\n","#     seed: 1004\n","#     model_arc: 'distilbert'\n","#     num_classes: 2\n","#     input_dir: '../input/nbme-score-clinical-patient-notes'\n","#     output_dir: './results/'\n","#     train_only: False\n","#     max_len: 512\n","#     ckp_path: '../input/nbme-hf-distilbert-2train/train/nbme-case/checkpoint-10432/'\n","#     train_args:\n","#         num_epochs: 7\n","#         train_batch_size: 32\n","#         val_batch_size: 32\n","#         model_path: 'pytorch_model.bin'\n","#         dropout_rate: 0.2 # 0.1~0.3\n","#         max_grad_norm: 1.0\n","#         max_lr: 0.0001\n","#         min_lr: 0.00001\n","#         cycle: 3\n","#         gamma: 0.5\n","#         weight_decay: 0.000001\n","#         log_intervals: 10\n","#         eval_metric: 'accuracy'\n","#         n_splits: 5"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:19.907795Z","iopub.status.busy":"2022-05-26T10:01:19.906493Z","iopub.status.idle":"2022-05-26T10:01:19.949834Z","shell.execute_reply":"2022-05-26T10:01:19.949015Z","shell.execute_reply.started":"2022-05-26T10:01:19.907755Z"},"trusted":true},"outputs":[],"source":["cfg = YamlConfigManager()\n","SEED = cfg.values.seed\n","MODEL_ARC = cfg.values.model_arc\n","INPUT_DIR = cfg.values.input_dir\n","OUTPUT_DIR = cfg.values.output_dir\n","TRAIN_ONLY = cfg.values.train_only\n","MAX_LEN = cfg.values.max_len\n","TOKENIZER = tokenizers.BertWordPieceTokenizer(f\"{cfg.values.ckp_path}/vocab.txt\", lowercase = True)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:19.951567Z","iopub.status.busy":"2022-05-26T10:01:19.951124Z","iopub.status.idle":"2022-05-26T10:01:19.957309Z","shell.execute_reply":"2022-05-26T10:01:19.956095Z","shell.execute_reply.started":"2022-05-26T10:01:19.951529Z"},"trusted":true},"outputs":[],"source":["yaml_dict = dict(cfg.values)\n","yaml_dict['train_args']['train_batch'] = 16\n","cfg.update(yaml_dict)"]},{"cell_type":"markdown","metadata":{},"source":["### fix SEED"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:19.959627Z","iopub.status.busy":"2022-05-26T10:01:19.958781Z","iopub.status.idle":"2022-05-26T10:01:19.968582Z","shell.execute_reply":"2022-05-26T10:01:19.967762Z","shell.execute_reply.started":"2022-05-26T10:01:19.959574Z"},"trusted":true},"outputs":[],"source":["def seed_everything(seed=1004):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:19.973335Z","iopub.status.busy":"2022-05-26T10:01:19.972788Z","iopub.status.idle":"2022-05-26T10:01:19.981604Z","shell.execute_reply":"2022-05-26T10:01:19.980634Z","shell.execute_reply.started":"2022-05-26T10:01:19.973252Z"},"trusted":true},"outputs":[],"source":["seed_everything(SEED)"]},{"cell_type":"markdown","metadata":{},"source":["### Load Data"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:19.983872Z","iopub.status.busy":"2022-05-26T10:01:19.982921Z","iopub.status.idle":"2022-05-26T10:01:20.370865Z","shell.execute_reply":"2022-05-26T10:01:20.370089Z","shell.execute_reply.started":"2022-05-26T10:01:19.983828Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n","feature_df = pd.read_csv(os.path.join(INPUT_DIR, 'features.csv'))\n","pn_df = pd.read_csv(os.path.join(INPUT_DIR, 'patient_notes.csv'))\n","test_df = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\n","submission_df = pd.read_csv(os.path.join(INPUT_DIR, 'sample_submission.csv'))"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:20.372556Z","iopub.status.busy":"2022-05-26T10:01:20.372182Z","iopub.status.idle":"2022-05-26T10:01:20.424522Z","shell.execute_reply":"2022-05-26T10:01:20.423696Z","shell.execute_reply.started":"2022-05-26T10:01:20.372515Z"},"trusted":true},"outputs":[],"source":["df = pd.merge(train_df, feature_df, on=['feature_num','case_num'], how='inner')\n","df = pd.merge(df, pn_df, on=['pn_num','case_num'], how='inner')\n","df.sample(5, random_state=SEED)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:20.426248Z","iopub.status.busy":"2022-05-26T10:01:20.425870Z","iopub.status.idle":"2022-05-26T10:01:20.448666Z","shell.execute_reply":"2022-05-26T10:01:20.446126Z","shell.execute_reply.started":"2022-05-26T10:01:20.426211Z"},"trusted":true},"outputs":[],"source":["df['feature_text'].value_counts()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:20.450478Z","iopub.status.busy":"2022-05-26T10:01:20.450077Z","iopub.status.idle":"2022-05-26T10:01:20.923222Z","shell.execute_reply":"2022-05-26T10:01:20.922303Z","shell.execute_reply.started":"2022-05-26T10:01:20.450441Z"},"trusted":true},"outputs":[],"source":["df[\"annotation\"] = [literal_eval(x) for x in df[\"annotation\"]]\n","df[\"location\"] = [literal_eval(x) for x in df[\"location\"]]\n","df.sample(5, random_state=SEED)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:20.925193Z","iopub.status.busy":"2022-05-26T10:01:20.924680Z","iopub.status.idle":"2022-05-26T10:01:20.931879Z","shell.execute_reply":"2022-05-26T10:01:20.931184Z","shell.execute_reply.started":"2022-05-26T10:01:20.925145Z"},"trusted":true},"outputs":[],"source":["def loc_list_to_ints(loc_list):\n","    to_return = []\n","    for loc_str in loc_list:\n","        loc_strs = loc_str.split(\";\")\n","        for loc in loc_strs:\n","            start, end = loc.split()\n","            to_return.append((int(start), int(end)))\n","    return to_return"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:20.933505Z","iopub.status.busy":"2022-05-26T10:01:20.933039Z","iopub.status.idle":"2022-05-26T10:01:20.952139Z","shell.execute_reply":"2022-05-26T10:01:20.951320Z","shell.execute_reply.started":"2022-05-26T10:01:20.933470Z"},"trusted":true},"outputs":[],"source":["def preprocess(pn_history, feature_text, annotation, location):\n","    \n","    location_list = loc_list_to_ints(location)        \n","    char_targets = [0] * len(pn_history) \n","    \n","    for loc,anno in zip(location_list, annotation): \n","        len_st = loc[1] - loc[0]\n","        idx0 = None\n","        idx1 = None\n","        for ind in (i for i, e in enumerate(pn_history) if (e == anno[0] and i == loc[0])):\n","            if pn_history[ind: ind + len_st] == anno:\n","                idx0 = ind\n","                idx1 = ind + len_st - 1\n","                if idx0 != None and idx1 != None:\n","                    for ct in range(idx0, idx1 + 1):\n","                        char_targets[ct] = 1  \n","                break\n","      \n","    tokenized_input = TOKENIZER.encode(feature_text, pn_history)\n","    \n","    input_ids = tokenized_input.ids\n","    mask = tokenized_input.attention_mask\n","    token_type_ids = tokenized_input.type_ids\n","    offsets = tokenized_input.offsets\n","    \n","    target_idx = []\n","    for j, (offset1, offset2) in enumerate(offsets):\n","        if sum(char_targets[offset1: offset2]) > 0:\n","            target_idx.append(j)\n","            \n","    #padding\n","    padding_length = MAX_LEN - len(input_ids)\n","    if padding_length > 0:\n","        input_ids = input_ids + ([0] * padding_length)\n","        mask = mask + ([0] * padding_length)\n","        token_type_ids = token_type_ids + ([0] * padding_length)\n","        offsets = offsets + ([(0, 0)] * padding_length)\n","       \n","    #creating label\n","    ignore_idxes = np.where(np.array(token_type_ids) != 1)[0]\n","\n","    label = np.zeros(len(offsets))\n","    label[ignore_idxes] = -1\n","    label[target_idx] = 1\n","\n","    return {\n","        'ids': input_ids,\n","        'mask': mask,\n","        'token_type_ids': token_type_ids,\n","        'labels': label,\n","        'offsets': offsets\n","    }"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:20.954412Z","iopub.status.busy":"2022-05-26T10:01:20.953671Z","iopub.status.idle":"2022-05-26T10:01:20.972071Z","shell.execute_reply":"2022-05-26T10:01:20.971173Z","shell.execute_reply.started":"2022-05-26T10:01:20.954365Z"},"trusted":true},"outputs":[],"source":["class NBMEDataset(torch.utils.data.Dataset):\n","    def __init__(self, df):\n","        super().__init__()\n","        self.df = df.reset_index()\n","        self.pn_history = df.pn_history.values\n","        self.feature_text = df.feature_text.values\n","        self.annotation = df.annotation.values\n","        self.location = df.location.values\n","        \n","    def __len__(self):\n","        return len(self.pn_history)\n","    \n","    def __getitem__(self, item):\n","        data = preprocess(\n","            self.pn_history[item],\n","            self.feature_text[item],\n","            self.annotation[item],\n","            self.location[item]\n","        )\n","        \n","        return {\n","            'ids': torch.tensor(data['ids'], dtype=torch.long),\n","            'mask': torch.tensor(data['mask'], dtype=torch.long),\n","            'token_type_ids': torch.tensor(data['token_type_ids'], dtype=torch.long),\n","            'labels': torch.tensor(data['labels'], dtype=torch.long),\n","            'offsets': torch.tensor(data['offsets'], dtype=torch.long)\n","        }"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:20.974359Z","iopub.status.busy":"2022-05-26T10:01:20.973740Z","iopub.status.idle":"2022-05-26T10:01:20.985455Z","shell.execute_reply":"2022-05-26T10:01:20.984453Z","shell.execute_reply.started":"2022-05-26T10:01:20.974319Z"},"trusted":true},"outputs":[],"source":["def get_dataloader(df, batch_size, shuffle):\n","    dataset = NBMEDataset(df=df)\n","    \n","    dataloader = torch.utils.data.DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        num_workers=4\n","    )\n","    \n","    return dataloader"]},{"cell_type":"markdown","metadata":{},"source":["## Model\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:20.990413Z","iopub.status.busy":"2022-05-26T10:01:20.990086Z","iopub.status.idle":"2022-05-26T10:01:21.077374Z","shell.execute_reply":"2022-05-26T10:01:21.076319Z","shell.execute_reply.started":"2022-05-26T10:01:20.990376Z"},"trusted":true},"outputs":[],"source":["class NBMEModel(transformers.DistilBertModel):\n","    def __init__(self, conf):\n","        super(NBMEModel, self).__init__(conf)\n","        self.pretrained_model = transformers.DistilBertModel.from_pretrained(cfg.values.ckp_path, config=conf)\n","        self.dropout = torch.nn.Dropout(cfg.values.train_args.dropout_rate)\n","        self.classifier = torch.nn.Linear(768, 1)\n","        torch.nn.init.normal_(self.classifier.weight, std=0.02)\n","    \n","    def forward(self, ids, mask, token_type_ids):\n","        sequence_output = self.pretrained_model(\n","            input_ids=ids, \n","            attention_mask=mask,\n","#             token_type_ids=token_type_ids\n","        )[0]\n","        \n","        sequence_output = self.dropout(sequence_output)\n","        \n","        logits = self.classifier(sequence_output)\n","        logits = logits.squeeze(-1)\n","        \n","        return logits"]},{"cell_type":"markdown","metadata":{},"source":["### AverageMeter"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:21.090191Z","iopub.status.busy":"2022-05-26T10:01:21.089454Z","iopub.status.idle":"2022-05-26T10:01:21.100332Z","shell.execute_reply":"2022-05-26T10:01:21.099531Z","shell.execute_reply.started":"2022-05-26T10:01:21.090153Z"},"trusted":true},"outputs":[],"source":["class AverageMeter():\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"markdown","metadata":{},"source":["### Loss"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:21.104457Z","iopub.status.busy":"2022-05-26T10:01:21.103561Z","iopub.status.idle":"2022-05-26T10:01:21.111645Z","shell.execute_reply":"2022-05-26T10:01:21.110644Z","shell.execute_reply.started":"2022-05-26T10:01:21.104418Z"},"trusted":true},"outputs":[],"source":["class ComputeMetric(object):\n","    def __init__(self, metric='bce') -> None:\n","        super().__init__()\n","        self.metric = metric\n","\n","    def compute_loss(self, logits, labels):\n","        if self.metric == 'bce':\n","            loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n","            loss = loss_fct(logits, labels)\n","        return loss"]},{"cell_type":"markdown","metadata":{},"source":["## Train\n","- 5 Fold Cross validation\n","- train each model with different learning rate and epoch"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:21.113479Z","iopub.status.busy":"2022-05-26T10:01:21.113067Z","iopub.status.idle":"2022-05-26T10:01:21.173309Z","shell.execute_reply":"2022-05-26T10:01:21.172029Z","shell.execute_reply.started":"2022-05-26T10:01:21.113442Z"},"trusted":true},"outputs":[],"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:21.202557Z","iopub.status.busy":"2022-05-26T10:01:21.201995Z","iopub.status.idle":"2022-05-26T10:01:21.238722Z","shell.execute_reply":"2022-05-26T10:01:21.237902Z","shell.execute_reply.started":"2022-05-26T10:01:21.202517Z"},"trusted":true},"outputs":[],"source":["def train(cfg, fold, train_loader, valid_loader):\n","        \n","    # Set train arguments\n","    num_epochs = cfg.values.train_args.num_epochs\n","    log_intervals = cfg.values.train_args.log_intervals\n","    max_lr = cfg.values.train_args.max_lr\n","    min_lr = cfg.values.train_args.min_lr\n","    cycle = cfg.values.train_args.cycle\n","    gamma = cfg.values.train_args.gamma\n","    weight_decay = cfg.values.train_args.weight_decay\n","    ckp_path = cfg.values.ckp_path\n","    max_grad_norm = cfg.values.train_args.max_grad_norm\n","    train_batch_size = cfg.values.train_args.train_batch_size\n","    val_batch_size = cfg.values.train_args.val_batch_size\n","    \n","    # Load model\n","    model_config = transformers.DistilBertConfig.from_pretrained(os.path.join(ckp_path, 'config.json'))\n","    model_config.output_hidden_states = True\n","    model = NBMEModel(conf=model_config)\n","    model.to(DEVICE)\n","    \n","    num_train_steps = int(len(train_loader) / train_batch_size * num_epochs)\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","    ]\n","    \n","    # Set optimizer and scheduler\n","    optimizer = AdamP(model.parameters(), \n","                      lr=max_lr, \n","                      weight_decay=weight_decay)\n","    first_cycle_steps = len(train_loader) * num_epochs // cycle\n","    scheduler = CosineAnnealingWarmRestarts(\n","        optimizer,\n","        T_0=first_cycle_steps,\n","        eta_min=min_lr,\n","    )\n","    \n","    eval_metric = ComputeMetric(metric='bce')\n","    best_loss = np.inf\n","    \n","    os.makedirs(os.path.join(OUTPUT_DIR, MODEL_ARC), exist_ok=True)\n","    \n","    # Train num_epochs times\n","    for epoch in range(num_epochs):\n"," \n","        model.train()\n","        since = time.time()\n","        loss_values = AverageMeter()\n","        \n","        for idx, train_batch in enumerate(tqdm(train_loader, desc=f'Train')):\n","            \n","            ids = train_batch['ids'].to(DEVICE, dtype=torch.long)\n","            mask = train_batch['mask'].to(DEVICE, dtype=torch.long)\n","            token_type_ids = train_batch['token_type_ids'].to(DEVICE, dtype=torch.long)\n","            offsets = train_batch['offsets'].to(DEVICE, dtype=torch.long)\n","            labels = train_batch['labels'].to(DEVICE, dtype=torch.float64)\n","            \n","            model.zero_grad()\n","            logits = model(ids=ids, \n","                           mask=mask,\n","                           token_type_ids=token_type_ids) #last_hidden_state\n","            \n","            # measure evaluation metric and record loss\n","            loss = eval_metric.compute_loss(logits, labels)\n","            loss = torch.masked_select(loss, labels > -1.0).mean()\n","            loss_values.update(loss.item(), ids.size(0))\n","            loss.requires_grad_(True)\n","            loss.backward()\n","\n","            # compute gradient and do optimizer step\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","            optimizer.step()\n","            scheduler.step()\n","\n","            if WANDB:\n","                wandb.log({\n","                    \"epoch\": epoch,\n","                    \"lr\": scheduler.get_lr()[0],\n","                    \"loss\": loss, \n","                    \"logits\": wandb.Histogram(logits.cpu().detach().numpy()),\n","                })\n","            \n","            if idx % log_intervals == 0:\n","                current_lr = scheduler.get_lr()[0]\n","                time_elapsed = time.time() - since\n","                tqdm.write(f\"Epoch : [{epoch + 1} / {num_epochs}][{idx}/{len(train_loader)}] || \"\n","                           f\"LR : {current_lr:.5f} || \"\n","                           f\"Train Loss : {loss_values.val:.4f} ({loss_values.avg:.4f}) || \"\n","                           f\"Training completed in {time_elapsed % 60:.0f}s\"\n","                          )\n","    \n","        if not TRAIN_ONLY:\n","            \n","            since = time.time()\n","            \n","            with torch.no_grad():\n","                model.eval()\n","                loss_values = AverageMeter()\n","\n","                for idx, val_batch in enumerate(tqdm(valid_loader, desc=f\"Validation\")):\n","\n","                    ids = val_batch['ids'].to(DEVICE, dtype=torch.long)\n","                    mask = val_batch['mask'].to(DEVICE, dtype=torch.long)\n","                    token_type_ids = val_batch['token_type_ids'].to(DEVICE, dtype=torch.long)\n","                    offsets = val_batch['offsets'].to(DEVICE, dtype=torch.long)\n","                    labels = val_batch['labels'].to(DEVICE, dtype=torch.float64)\n","\n","                    model.zero_grad()\n","                    logits = model(ids=ids, \n","                                   mask=mask, \n","                                   token_type_ids=token_type_ids) #last_hidden_state\n","\n","                    # measure evaluation metric and record loss\n","                    loss = eval_metric.compute_loss(logits, labels)\n","                    loss = torch.masked_select(loss, labels > -1.0).mean()\n","                    loss_values.update(loss.item(), ids.size(0))\n","\n","            time_elapsed = time.time() - since\n","            tqdm.write(f\"Epoch : [{epoch + 1} / {num_epochs}] || \"\n","                       f\"Val Loss : {loss_values.avg:.4f} || \"\n","                       f\"Validation completed in {time_elapsed % 60:.0f}s\"\n","                      )\n","\n","            is_best = loss_values.avg < best_loss\n","            best_loss = max(loss_values.avg, best_loss)\n","\n","            if is_best:\n","                if fold > 0:\n","                    os.makedirs(os.path.join(OUPUT_DIR, MODEL_ARC, f\"{k}_fold\"), exist_ok=True)\n","                    torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, MODEL_ARC, f\"{fold}_fold\", f\"{epoch+1}_epoch_{best_loss:.2f}%_with_val.pth\"))\n","            else:\n","                torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, MODEL_ARC, f\"{epoch+1}_epoch_{best_loss:.2f}%_with_val.pth\"))   "]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:21.240549Z","iopub.status.busy":"2022-05-26T10:01:21.240029Z","iopub.status.idle":"2022-05-26T10:01:21.256196Z","shell.execute_reply":"2022-05-26T10:01:21.255494Z","shell.execute_reply.started":"2022-05-26T10:01:21.240515Z"},"trusted":true},"outputs":[],"source":["def run(cfg, df):    \n","    # Set train arguments\n","    n_splits = cfg.values.train_args.n_splits\n","    train_batch_size = cfg.values.train_args.train_batch_size\n","    val_batch_size = cfg.values.train_args.val_batch_size\n","    \n","    # Train on K-fold\n","    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n","\n","    for fold, (train_idx, val_idx) in enumerate(kfold.split(df)):\n","        print('\\n')\n","        cpprint('*' * 15 + f\" {fold + 1}-Fold Cross Validation \" + '*' * 15)\n","\n","        train_df = df.iloc[train_idx]\n","        val_df = df.iloc[val_idx]\n","\n","        train_loader = get_dataloader(\n","            df=train_df, \n","            batch_size=train_batch_size, \n","            shuffle=True\n","        )\n","        \n","        val_loader = get_dataloader(\n","            df=val_df,\n","            batch_size=val_batch_size,\n","            shuffle=False\n","        )\n","    \n","        train(cfg, fold, train_loader, val_loader)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:01:21.258069Z","iopub.status.busy":"2022-05-26T10:01:21.257549Z","iopub.status.idle":"2022-05-26T10:07:38.265473Z","shell.execute_reply":"2022-05-26T10:07:38.263141Z","shell.execute_reply.started":"2022-05-26T10:01:21.258034Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()\n","run(cfg, df)"]},{"cell_type":"markdown","metadata":{},"source":["## Inference"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:07:42.526337Z","iopub.status.busy":"2022-05-26T10:07:42.525926Z","iopub.status.idle":"2022-05-26T10:07:42.566672Z","shell.execute_reply":"2022-05-26T10:07:42.565729Z","shell.execute_reply.started":"2022-05-26T10:07:42.526303Z"},"trusted":true},"outputs":[],"source":["df_tst = pd.merge(test_df, feature_df, on=['feature_num','case_num'], how='inner')\n","df_tst = pd.merge(df_tst, pn_df, on=['pn_num','case_num'], how='inner')\n","df_tst.shape"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:29:41.851207Z","iopub.status.busy":"2022-05-26T10:29:41.850818Z","iopub.status.idle":"2022-05-26T10:29:41.864492Z","shell.execute_reply":"2022-05-26T10:29:41.863318Z","shell.execute_reply.started":"2022-05-26T10:29:41.851175Z"},"trusted":true},"outputs":[],"source":["def test_preprocess(pn_history, feature_text):\n","      \n","    tokenized_input = TOKENIZER.encode(feature_text, pn_history)\n","    \n","    input_ids = tokenized_input.ids\n","    mask = tokenized_input.attention_mask\n","    token_type_ids = tokenized_input.type_ids\n","    offsets = tokenized_input.offsets\n","            \n","    #padding\n","    padding_length = MAX_LEN - len(input_ids)\n","    if padding_length > 0:\n","        input_ids = input_ids + ([0] * padding_length)\n","        mask = mask + ([0] * padding_length)\n","        token_type_ids = token_type_ids + ([0] * padding_length)\n","        offsets = offsets + ([(0, 0)] * padding_length)\n","\n","    return {\n","        'ids': input_ids,\n","        'mask': mask,\n","        'token_type_ids': token_type_ids,\n","        'offsets': offsets\n","    }"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:29:46.217223Z","iopub.status.busy":"2022-05-26T10:29:46.216841Z","iopub.status.idle":"2022-05-26T10:29:46.228044Z","shell.execute_reply":"2022-05-26T10:29:46.227091Z","shell.execute_reply.started":"2022-05-26T10:29:46.217191Z"},"trusted":true},"outputs":[],"source":["class TestDataset(torch.utils.data.Dataset):\n","    def __init__(self, df):\n","        super().__init__()\n","        self.df = df.reset_index()\n","        self.pn_history = df.pn_history\n","        self.feature_text = df.feature_text\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        data = test_preprocess(\n","            self.pn_history[idx],\n","            self.feature_text[idx],\n","        )\n","        \n","        return {\n","            'ids': torch.tensor(data['ids'], dtype=torch.long),\n","            'mask': torch.tensor(data['mask'], dtype=torch.long),\n","            'token_type_ids': torch.tensor(data['token_type_ids'], dtype=torch.long),\n","            'offsets': torch.tensor(data['offsets'], dtype=torch.long)\n","        }"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:55:20.350870Z","iopub.status.busy":"2022-05-26T10:55:20.350453Z","iopub.status.idle":"2022-05-26T10:55:20.364449Z","shell.execute_reply":"2022-05-26T10:55:20.363040Z","shell.execute_reply.started":"2022-05-26T10:55:20.350837Z"},"trusted":true},"outputs":[],"source":["# def logit_to_char_targets(avg_logit, test_batch.offsets):\n","    \n","#     return '0 0'"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2022-05-26T10:59:57.151341Z","iopub.status.busy":"2022-05-26T10:59:57.150901Z","iopub.status.idle":"2022-05-26T11:00:04.041071Z","shell.execute_reply":"2022-05-26T11:00:04.036855Z","shell.execute_reply.started":"2022-05-26T10:59:57.151304Z"},"trusted":true},"outputs":[],"source":["# # (Manually) select model checkpoints for prediction\n","# model_ckps = [\n","#     './results/distilbert/1_epoch_inf%_with_val.pth',\n","# ]\n","# model_config = transformers.BertConfig.from_pretrained(cfg.values.ckp_path)\n","# model_config.output_hidden_states = True\n","# model = NBMEModel(conf=model_config)\n","\n","\n","# # Prepare Test DataLoader\n","# test_dataset = TestDataset(df_tst)\n","\n","# test_dataloader = torch.utils.data.DataLoader(\n","#     test_dataset,\n","#     shuffle=False,\n","#     batch_size=cfg.values.train_args.train_batch_size,\n","#     num_workers=1\n","# )\n","\n","\n","# # Predict on Test DataLoader\n","# avg_logits_list = []\n","# save_ = []\n","\n","# with torch.no_grad():\n","#     tk = tqdm(test_dataloader, total=len(test_dataloader)) \n","    \n","#     test_logits = []\n","#     for idx, test_batch in enumerate(tk):\n","#         ids = test_batch['ids'].to(DEVICE, dtype=torch.long)\n","#         mask = test_batch[\"mask\"].to(DEVICE, dtype=torch.long)\n","#         token_type_ids = test_batch[\"token_type_ids\"].to(DEVICE, dtype=torch.long)\n","        \n","#         for ckp, model_ckp in enumerate(model_ckps):\n","#             model.load_state_dict(torch.load(model_ckp))\n","#             model.to(DEVICE)\n","#             model.eval()\n","            \n","#             logits = model(ids=ids, \n","#                            mask=mask, \n","#                            token_type_ids=token_type_ids\n","#                     ) #last_hidden_state\n","            \n","#             test_logits.append(logits.cpu().detach().numpy())\n","        \n","#         avg_logits = np.mean(test_logits, axis=0)\n","#         save_.append(avg_logits)\n","\n","#     save_ = np.concatenate(save_)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Soft Ensemble\n","# preds = save_.argmax(-1)\n","# preds = [logit_to_char_targets(pred) for pred in preds]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-26T10:55:56.119493Z","iopub.status.idle":"2022-05-26T10:55:56.119873Z","shell.execute_reply":"2022-05-26T10:55:56.119717Z","shell.execute_reply.started":"2022-05-26T10:55:56.119700Z"},"trusted":true},"outputs":[],"source":["# preds.shape # len(submission_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-26T10:07:38.281649Z","iopub.status.idle":"2022-05-26T10:07:38.282578Z","shell.execute_reply":"2022-05-26T10:07:38.282335Z","shell.execute_reply.started":"2022-05-26T10:07:38.282308Z"},"trusted":true},"outputs":[],"source":["# submission_df['location'] = preds\n","# submission_df.to_csv(f'submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
